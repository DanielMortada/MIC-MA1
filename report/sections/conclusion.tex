% =====================================
% Conclusion Section
% File: sections/conclusion.tex
% =====================================

\section{Conclusion}
\noindent
We investigated musical‑instrument image classification in three phases. Phase~1 showed the strength of transfer learning (ResNet‑18, 99.33\%) over a scratch CNN (80.67\%). Phase~2 demonstrated that added depth (Deeper CNN, 86.67\%) beats width or heavy regularisation. Phase~3 confirmed that naive stacking of advanced tricks can hurt (81\%), but failure‑driven refinements – selective attention, lighter dropout, class‑specific augmentation, mixed precision – lifted the custom model to 93.33\%, only 6\% below the baseline.

\noindent
Key contributions are:
\begin{itemize}
    \item A reproducible pipeline for fair comparison of transfer learning and bespoke CNNs on a 30‑class instrument set.
    \item Evidence that depth is the most effective custom‑architecture lever, whereas over‑regularisation degrades learning.
    \item A failure‑analysis workflow that turns regression into a 12.33\% gain via targeted augmentation and balanced regularisation.
    \item Design lessons – class‑specific augmentation, layer‑selective attention, and moderate dropout – that generalise to similar fine‑grained vision tasks.
\end{itemize}

\noindent
Attempting multiple complex changes at once can destabilise training, whereas iterative, data‑driven tuning delivers steady gains. Diligent design and systematic optimisation  -  informed by both successes and failures  -  can push custom CNNs close to high‑performing transfer‑learning baselines. 
