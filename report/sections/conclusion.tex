% Conclusion Section
\section{Conclusion}
This paper presented a comprehensive three-phase investigation into musical instrument classification using CNNs. Phase 1 contrasted a ResNet-18 transfer learning baseline (99.33\% test accuracy) with a custom CNN (80.67\% accuracy). While transfer learning offered superior performance and faster convergence, the custom model provided domain-specific insights and a compact footprint.

Phase 2 systematically compared architectural variants using a flexible framework. The Deeper CNN (86.67\% accuracy) outperformed Base (85.33\%), Regularized (81.33\%), and Wider (80.67\%) variants, suggesting depth was more beneficial than width for this task and highlighting the need for balanced regularization.

Phase 3 meticulously optimized the Deeper CNN by integrating selective attention, residual connections, graduated dropout, class-specific augmentation, and mixed precision training. These combined enhancements yielded a final test accuracy of 93.33\%, a significant 6.66\% improvement over its Phase 2 performance, narrowing the gap with the ResNet-18 baseline to 6.00\%.

Collectively, our work offers several key contributions:
\begin{itemize}
    \item A systematic methodology for developing, comparing, and optimizing custom CNNs for specialized tasks, including strategies for class imbalance.
    \item Empirical support for applying selective attention primarily to deeper layers for targeted improvements without excessive computational cost.
    \item Insights into architectural component importance for this task: depth proved more advantageous than width, and graduated dropout outperformed uniform regularization.
    \item Identification of challenging classes (e.g., Alphorn, Flute) and validation of targeted augmentation strategies to mitigate difficulties.
    \item Interpretable insights via visualization into how architectural enhancements improve feature discrimination.
\end{itemize}

The study underscores the value of incremental optimization, as attempting multiple complex changes simultaneously could destabilize training, whereas methodical refinement yielded substantial gains.

In conclusion, this research demonstrates that diligent design and systematic optimization can enable custom CNNs to approach transfer learning performance on challenging visual tasks. The validated techniques—particularly selective attention, residual connections, and graduated dropout—offer valuable design principles applicable to other image classification domains.
