% =====================================
% Dataset and Experimental Setup Section
% File: sections/dataset.tex
% =====================================

\section{Dataset and Experimental Setup}

\subsection{Dataset}
\noindent
We use the Kaggle musical‑instrument image set \cite{gpiosenka2021} comprising 30 classes. After duplicate removal, each image is resized to $224 \times 224 \times 3$ RGB and stored as JPG. For every class, the data are split into a training set, five‑image validation set, and five‑image test set.

\noindent
This collection poses real‑world challenges: variable pose, lighting, and background; occasional players causing occlusion; inter‑instrument diversity in size and finish; and limited samples for some classes (\autoref{fig:sampleimages}).

\begin{figure}[htbp]
    \centering
    \subfloat[Ocarina]{%
        \includegraphics[width=0.41\linewidth]{figures/sample_data_image.png}%
    }
    \subfloat[Violin]{%
        \includegraphics[width=0.41\linewidth]{figures/sample data image 1.png}%
    }\\[1ex]
    \subfloat[Didgeridoo]{%
        \includegraphics[width=0.41\linewidth]{figures/sample data image 2.png}%
    }
    \subfloat[Clarinet]{%
        \includegraphics[width=0.41\linewidth]{figures/sample data image 3.png}%
    }
    \caption{Example images from the 30-class musical instrument dataset. Each sample illustrates class variability in pose, background, and appearance.}
    \label{fig:sampleimages}
\end{figure}



\subsection{Pre‑processing and Augmentation}
\noindent
Images are normalized with ImageNet statistics (mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]), resized to $224 \times 224$, and augmented during training by random horizontal flips, rotations ($\pm 10^\circ$), color jitter, and slight random crops. These operations enlarge the effective training set and improve generalization.

\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{figures/training_post_augmentation.png}
    \caption{Illustration of the medium-strength data augmentation pipeline applied to the musical instrument dataset. The augmentation process includes random rotations, horizontal flips, color jittering, and random affine transformations.}
    \label{fig:training_augmentation}
\end{figure}



\subsection{Experimental Setup}
\noindent
All models are implemented in PyTorch and trained on a GPU. Unless noted, we use cross‑entropy loss, the Adam optimizer (initial learning rate 0.001), batch size 32, and early stopping with a patience of 10 epochs, evaluating performance with accuracy.

\noindent
The ResNet‑18 baseline is fine‑tuned from ImageNet weights, whereas custom CNNs start from He‑initialized random weights \cite{he2015delving}. Identical pre‑processing, optimization, and evaluation protocols are applied across phases to ensure fair comparison.
