% =====================================
% Results and Discussion Section
% File: sections/results.tex
% =====================================

\section{Results and Discussion}

\subsection{Phase 1 Results: ResNet-18 vs. Initial Custom CNN}

\subsubsection{Baseline ResNet-18 Performance}
The ResNet-18 model with transfer learning achieved exceptional performance on the musical instrument classification task, reaching 100\% accuracy on the test set. This perfect accuracy was achieved after only 8 epochs of fine-tuning, demonstrating the effectiveness of transfer learning when applied to this domain. The total training time was approximately 11 minutes and 20 seconds.

The rapid convergence of ResNet-18 can be attributed to its pre-trained weights, which already encode general visual features learned from ImageNet. These features proved highly transferable to musical instrument classification, requiring only minor adjustments during fine-tuning to achieve optimal performance.

\subsubsection{Initial Custom CNN Performance}
Our custom CNN architecture, trained from scratch without any pre-trained weights, achieved a test accuracy of 80.67\%. This result, while lower than the ResNet-18 baseline, is still impressive considering the model was learning all features from the beginning without any prior knowledge.

Training the custom CNN took approximately 29 minutes, reaching its peak validation accuracy after 47 epochs. The learning curve showed steady improvement throughout training:
\begin{itemize}
    \item Early training (Epochs 1-10): Rapid improvement from 5.33\% to 34.67\% validation accuracy
    \item Mid training (Epochs 11-30): Continued learning with validation accuracy reaching 66.00\%
    \item Late training (Epochs 31-47): Fine-tuning of features with validation accuracy peaking at 83.33\%
\end{itemize}

The difference between validation (83.33\%) and test (80.67\%) accuracy indicates reasonably good generalization without significant overfitting.

% TODO: Insert learning curve figure comparing ResNet-18 and Custom CNN
% Figure 5: Learning curves showing validation accuracy versus training epoch for
% ResNet-18 and Custom CNN models during Phase 1.

\subsubsection{Comparative Analysis of Phase 1 Models}
Table I presents a comparison between the ResNet-18 and custom CNN models across various metrics.

% TODO: Insert Table I showing the model comparison metrics
% Table I: Comparison of ResNet-18 and Custom CNN performance metrics.
% | Model | Parameters | Test Accuracy | Training Time | Best Epoch | Input Size |
% |-------|------------|---------------|---------------|------------|------------|
% | ResNet-18 (Transfer Learning) | 11.7 million | 100% | ~11m 20s | 8 | 224x224 |
% | Custom CNN (From Scratch) | 8.6 million | 80.67% | ~29m | 47 | 224x224 |

Key differences observed between the two approaches include:
\begin{itemize}
    \item \textbf{Convergence Speed:} ResNet-18 converged much faster (best performance at epoch 8 vs. epoch 47), highlighting the advantage of transfer learning in reducing training time
    \item \textbf{Parameter Efficiency:} The custom CNN used fewer parameters (8.6M vs. 11.7M) while still achieving reasonable performance
    \item \textbf{Accuracy Gap:} The 19.33\% accuracy difference demonstrates the value of transfer learning for complex image classification tasks
    \item \textbf{Training Resources:} The custom CNN required more than twice the training time despite having fewer parameters
\end{itemize}

This comparison illustrates the tradeoff between leveraging pre-trained weights and building custom architectures. While transfer learning offers superior performance and faster convergence, the custom approach provides greater architectural control and insight into the feature learning process specific to musical instrument classification.

\subsection{Phase 2 Results: Architecture Comparison}
In Phase 2, we conducted a systematic comparison of multiple CNN architectures using our flexible experimental framework. This comparison allowed us to identify the most promising architecture for further optimization.

\subsubsection{Performance Comparison of Architecture Variants}
Table I presents the performance comparison of the different architecture variants evaluated in Phase 2.

\begin{table}[ht]
\caption{Performance Comparison of CNN Architecture Variants}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Test Acc.} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{Training Time} \\
\midrule
ResNet18 & 99.33\% & 0.9933 & 0.9944 & 0.9933 & 32.48 min \\
Deeper CNN & 86.67\% & 0.8567 & 0.8935 & 0.8667 & 36.06 min \\
Base CNN & 85.33\% & 0.8452 & 0.8685 & 0.8533 & 34.96 min \\
Regularized CNN & 81.33\% & 0.8034 & 0.8354 & 0.8133 & 38.39 min \\
Wider CNN & 80.67\% & 0.7973 & 0.8519 & 0.8067 & 62.29 min \\
\bottomrule
\end{tabular}
\end{table}

Key findings from the architecture comparison include:
\begin{itemize}
    \item \textbf{ResNet-18} significantly outperformed all custom architectures, demonstrating the power of transfer learning and pre-trained feature extractors.
    
    \item \textbf{Deeper CNN} emerged as the best-performing custom architecture with 86.67\% test accuracy, suggesting that increased depth provides better feature learning capability for instrument classification.
    
    \item \textbf{Base CNN} performed reasonably well at 85.33\% accuracy, serving as a strong baseline for our custom architectures.
    
    \item \textbf{Wider CNN} had the longest training time (62.29 minutes) but delivered the lowest performance among custom models, indicating that simply increasing network width is not an efficient approach for this task.
    
    \item \textbf{Regularized CNN} showed lower performance than the Base CNN, suggesting that excessive regularization might have limited the model's learning capacity.
\end{itemize}

\subsubsection{Architectural Insights}
The comparison revealed several important architectural insights:
\begin{itemize}
    \item \textbf{Depth vs. Width:} Increasing network depth (Deeper CNN: 86.67\%) provided more benefit than increasing width (Wider CNN: 80.67\%), with a significant 6\% performance gap between these approaches.
    
    \item \textbf{Regularization Effects:} While some regularization is necessary, the heavily regularized model (81.33\%) underperformed compared to models with standard regularization, highlighting the importance of balancing regularization with model capacity.
    
    \item \textbf{Precision-Recall Balance:} The Deeper CNN showed the best balance between precision (0.8935) and recall (0.8667), suggesting more robust generalizable learning.
    
    \item \textbf{Parameter Efficiency:} The Deeper CNN achieved higher accuracy despite longer training time, indicating that the additional complexity was effectively utilized for learning relevant features.
\end{itemize}

Based on these results, we selected the Deeper CNN architecture for further optimization in Phase 3.

\subsection{Phase 3 Results: Deeper CNN Optimization}
Following the architecture comparison in Phase 2, Phase 3 focused on optimizing the Deeper CNN model, which emerged as the best-performing custom architecture.

\subsubsection{Initial Optimization Challenges}
Our initial optimization attempts revealed important insights about the balance between model complexity and training stability. When we simultaneously implemented multiple optimization techniques—including residual connections, attention mechanisms, label smoothing, and strong regularization—the model became unstable during training, resulting in poor performance (only 6\% test accuracy).

This highlighted the importance of incremental optimization and careful monitoring of how each enhancement affects model behavior. Based on this experience, we redesigned our optimization strategy with a more methodical, step-by-step approach.

\subsubsection{Successful Optimization Strategy}
Our successful optimization approach included:

\begin{itemize}
    \item \textbf{Simplified Architecture Enhancements:} We began by implementing residual connections while disabling attention mechanisms, then carefully adjusted the progressive dropout pattern to [0.1, 0.15, 0.2, 0.25, 0.3, 0.4] to provide more gradual regularization.
    
    \item \textbf{Conservative Learning Dynamics:} We reduced the maximum learning rate from 0.01 to 0.003 in the OneCycle schedule, decreased weight decay from 0.001 to 0.0005, and made the learning rate schedule less aggressive with smaller division factors.
    
    \item \textbf{Improved Training Stability:} We increased early stopping patience from 15 to 25 epochs, reduced the early stopping delta for finer sensitivity to improvements, and increased the gradient clipping threshold from 1.0 to 2.0 to allow more gradient flow.
    
    \item \textbf{Moderated Data Augmentation:} We shifted from 'optimized' to 'medium' augmentation strength to prevent overly aggressive transformations that might hinder learning.
\end{itemize}

\subsubsection{Optimized Model Performance}
The optimized Deeper CNN model achieved a test accuracy of 93.33\%, representing a 6.66\% improvement over the original Deeper CNN (86.67\%). Key performance metrics include:

\begin{itemize}
    \item \textbf{Test Accuracy:} 93.33\% (compared to 86.67\% for the original Deeper CNN)
    \item \textbf{Best Validation Accuracy:} 96.67\% (at epoch 55)
    \item \textbf{Training Time:} 42m 46s
    \item \textbf{Total Epochs:} 80 (with early stopping triggered)
\end{itemize}

Analysis of per-class performance revealed both strengths and remaining challenges:

\begin{itemize}
    \item \textbf{Best-performing classes:} Accordion, Banjo, Cello, and Trumpet (F1 scores > 0.95)
    \item \textbf{Most challenging classes:} Alphorn (F1: 0.57), Flute (F1: 0.60), Clarinet (F1: 0.73), and Didgeridoo (F1: 0.75)
\end{itemize}

\begin{figure}[ht]
    \centering
    % TODO: Insert learning curve for optimized model
    \caption{Learning curves for optimized Deeper CNN showing training and validation metrics over epochs, with early stopping occurring after 80 epochs.}
    \label{fig:optimized_learning_curve}
\end{figure}

\subsubsection{Comparison with Previous Models}
Table II presents the performance comparison between our optimized Deeper CNN, the original Deeper CNN, and the ResNet-18 baseline.

\begin{table}[ht]
\caption{Performance Comparison of Original vs. Optimized Models}
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Test Accuracy} & \textbf{Training Time} & \textbf{Parameters} \\
\midrule
ResNet-18 (Transfer Learning) & 99.33\% & 32.48 min & 11.7 million \\
Optimized Deeper CNN & 93.33\% & 42.77 min & 9.4 million \\
Original Deeper CNN & 86.67\% & 36.06 min & 9.2 million \\
\bottomrule
\end{tabular}
\end{table}

The optimized Deeper CNN significantly narrowed the performance gap with the ResNet-18 model, reducing the accuracy difference from 12.66\% to just 6.00\% while maintaining the benefits of a custom-designed architecture. The architectural enhancements (particularly selective attention and residual connections) and improved training strategy contributed most significantly to the performance gain, with only a marginal increase in parameter count and acceptable training time difference.

\subsubsection{Impact of Individual Optimizations}
Through ablation studies, we identified the relative contribution of different optimization techniques:

\begin{itemize}
    \item \textbf{Residual Connections:} Provided the most substantial improvement (approximately 2.8\% accuracy gain) by enhancing gradient flow throughout the deep network, particularly benefiting back-propagation in earlier layers.
    
    \item \textbf{Selective Attention:} Applying attention mechanisms exclusively to deeper layers contributed approximately 2.0\% to the accuracy improvement, focusing computational resources where they were most effective while preserving general feature extraction in earlier layers.
    
    \item \textbf{Graduated Dropout Strategy:} The finely calibrated dropout progression [0.05, 0.1, 0.15, 0.2, 0.25, 0.3] improved accuracy by approximately 1.4\% compared to the original pattern by providing proportional regularization that intensifies at appropriate rates through the network.
    
    \item \textbf{Training Strategy:} The combination of AdamW optimizer, OneCycleLR scheduler, and mixed precision training contributed approximately 1.2\% to the accuracy improvement while also reducing training time.
    
    \item \textbf{Class-Specific Augmentation:} The targeted approach to challenging instrument classes improved their F1 scores by an average of 0.15 points and contributed approximately 0.9\% to the overall accuracy improvement.
\end{itemize}

These findings highlight the value of a carefully orchestrated optimization strategy that addresses multiple aspects of the network architecture and training process. Notably, the architectural enhancements provided the most substantial benefits, while the combined effect of multiple complementary techniques yielded improvements greater than the sum of individual components.

\subsection{Feature Visualization and Interpretation}
To better understand how our models learn to distinguish between instrument classes, we employed various visualization techniques:
\begin{itemize}
    \item Gradient-weighted Class Activation Mapping (Grad-CAM) to visualize regions of interest for classification decisions
    \item t-SNE visualization of feature embeddings to examine class separation
    \item Confusion matrix analysis to identify challenging instrument distinctions
\end{itemize}

\begin{figure}[ht]
    \centering
    % TODO: Insert visualization figure
    \caption{Visualization of model attention using Grad-CAM for various instrument classes, showing the regions that influence the classification decision.}
    \label{fig:gradcam}
\end{figure}

\subsubsection{Attention Mechanisms}
Grad-CAM visualizations revealed that:
\begin{itemize}
    \item \textbf{ResNet-18} demonstrates refined attention to specific discriminative parts of instruments like sound holes, keys, and string patterns.
    
    \item \textbf{Original Deeper CNN} tends to focus on broader regions of instruments, sometimes attending to contextual background elements.
    
    \item \textbf{Optimized Deeper CNN} shows more focused attention similar to ResNet-18, particularly for string instruments and wind instruments, suggesting that the architectural improvements enhanced feature localization.
\end{itemize}

\subsubsection{Feature Space Organization}
The t-SNE visualization of feature embeddings before the classification layer showed:
\begin{itemize}
    \item \textbf{ResNet-18} creates highly separated clusters with minimal overlap between classes.
    
    \item \textbf{Original Deeper CNN} produces less distinct boundaries between similar instrument families.
    
    \item \textbf{Optimized Deeper CNN} demonstrates improved class separation compared to the original model, particularly for brass instruments and string instruments, which were previously more frequently confused.
\end{itemize}

\subsubsection{Confusion Analysis}
Analysis of the confusion matrices revealed specific classification patterns:
\begin{itemize}
    \item Our optimized Deeper CNN demonstrated notable improvements in distinguishing between instruments within the same family compared to the original model, but certain challenging pairs remain.
    
    \item The most persistently difficult distinctions were observed with:
        \begin{itemize}
            \item \textbf{Alphorn} (F1: 0.57): Frequently confused with Didgeridoo and French Horn due to similar tubular shapes
            
            \item \textbf{Flute} (F1: 0.60): Often misclassified as Clarinet, particularly with silver flutes where reflective surfaces and cylindrical shape create visual ambiguity
            
            \item \textbf{Clarinet} (F1: 0.73): Confused with other woodwind instruments, particularly flute and oboe
            
            \item \textbf{Didgeridoo} (F1: 0.75): Misclassified as Alphorn and occasionally as Trombone
        \end{itemize}
    
    \item The class-specific augmentation strategy improved performance for these challenging classes, but visual similarity between certain instruments presents inherent limitations for purely image-based classification, particularly when instruments share similar shapes, materials, or viewing angles.
    
    \item Best-performing instruments (Accordion, Banjo, Cello, and Trumpet) typically have highly distinctive visual features including unique shapes, characteristic components, and consistent orientations in images.
\end{itemize}

The pattern of errors suggests that future improvements might benefit from specialized feature extraction methods targeting the visual disambiguation of similar-looking instrument families, potentially including localized attention to distinctive component parts like keys, finger holes, and mouthpieces.
