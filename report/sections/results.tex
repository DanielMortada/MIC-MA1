% =====================================
% Results and Discussion Section
% File: sections/results.tex
% =====================================

\section{Results and Discussion}
This section presents the empirical findings derived from our three-phase investigation, detailing the performance of the baseline model, the comparative analysis of custom CNN architectures, and the outcomes of the final optimization phase.

\subsection{Phase 1 Results: ResNet-18 vs. Initial Custom CNN}

\subsubsection{Baseline ResNet-18 Performance}
The ResNet-18 model, leveraging transfer learning, exhibited exceptional performance on the musical instrument classification task. It achieved a test accuracy of 100\% after only 8 epochs of fine-tuning, requiring approximately 11 minutes and 20 seconds. This rapid convergence underscores the effectiveness of its pre-trained ImageNet features, which proved highly transferable and required minimal adaptation for this specific domain.

\subsubsection{Initial Custom CNN Performance}
In contrast, our custom CNN architecture, trained entirely from scratch, attained a test accuracy of 80.67\%. While numerically lower than the baseline, this result is nonetheless commendable, considering the model learned all relevant features autonomously from the provided dataset. The training process spanned approximately 29 minutes, with the model reaching its peak validation accuracy of 83.33\% at epoch 47.

The learning trajectory demonstrated steady progress:
\begin{itemize}
    \item During early training (Epochs 1-10), validation accuracy increased rapidly from 5.33\% to 34.67\%.
    \item Through mid-training (Epochs 11-30), learning continued steadily, reaching 66.00\% validation accuracy.
    \item In late training (Epochs 31-47), the model refined its feature representations, culminating in the peak validation accuracy.
\end{itemize}
The relatively narrow gap between the peak validation accuracy (83.33\%) and the final test accuracy (80.67\%) suggests reasonable generalization capabilities without evidence of significant overfitting.

% TODO: Insert learning curve figure comparing ResNet-18 and Custom CNN

\subsubsection{Comparative Analysis of Phase 1 Models}
Table I provides a comparative overview of the ResNet-18 and custom CNN models based on key performance metrics.

% TODO: Insert Table I: Comparison of ResNet-18 and Custom CNN performance metrics.
% | Model | Parameters | Test Accuracy | Training Time | Best Epoch | Input Size |
% |-------|------------|---------------|---------------|------------|------------|
% | ResNet-18 (Transfer Learning) | 11.7M | 100% | ~11m 20s | 8 | 224x224 |
% | Custom CNN (From Scratch) | 8.6M | 80.67% | ~29m | 47 | 224x224 |

Several key distinctions emerged from this initial comparison:
\begin{itemize}
    \item \textbf{Convergence Speed:} ResNet-18 converged substantially faster (best performance at epoch 8 vs. 47), clearly illustrating the training time advantage conferred by transfer learning.
    \item \textbf{Parameter Efficiency:} The custom CNN utilized fewer parameters (8.6M vs. 11.7M), indicating a more compact architecture despite its respectable performance.
    \item \textbf{Accuracy Discrepancy:} A significant 19.33\% difference in test accuracy highlights the substantial value of pre-trained features for this complex classification task.
    \item \textbf{Training Resources:} Despite its smaller parameter count, the custom CNN demanded more than double the training time compared to the fine-tuned ResNet-18.
\end{itemize}
This initial comparison underscores the fundamental tradeoff: while transfer learning yields superior accuracy and faster convergence, the custom approach offers greater architectural control and provides domain-specific feature learning insights.

\subsection{Phase 2 Results: Architectural Comparison}
Subsequently, Phase 2 involved a systematic comparison of various CNN architectures, facilitated by our flexible experimental framework designed for controlled evaluation.

\subsubsection{Performance Comparison of Architecture Variants}
Table I summarizes the performance metrics for the different architectural variants assessed during Phase 2.

\begin{table}[ht]
\caption{Performance Comparison of CNN Architecture Variants}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Test Acc.} & \textbf{F1 Score} & \textbf{Precision} & \textbf{Recall} & \textbf{Training Time} \\
\midrule
ResNet18 & 99.33\% & 0.9933 & 0.9944 & 0.9933 & 32.48 min \\
Deeper CNN & 86.67\% & 0.8567 & 0.8935 & 0.8667 & 36.06 min \\
Base CNN & 85.33\% & 0.8452 & 0.8685 & 0.8533 & 34.96 min \\
Regularized CNN & 81.33\% & 0.8034 & 0.8354 & 0.8133 & 38.39 min \\
Wider CNN & 80.67\% & 0.7973 & 0.8519 & 0.8067 & 62.29 min \\
\bottomrule
\end{tabular}
\end{table}

Key observations derived from this architectural comparison include:
\begin{itemize}
    \item \textbf{ResNet-18 Dominance:} The transfer learning approach consistently maintained a significant performance advantage over all custom-built architectures.
    \item \textbf{Deeper CNN Superiority:} Among the custom models, the Deeper CNN emerged as the top performer (86.67\% test accuracy), suggesting that increased network depth facilitated more effective feature learning for this task compared to other modifications.
    \item \textbf{Base CNN Viability:} The Base CNN provided a solid performance baseline (85.33\%), confirming the fundamental soundness of the initial custom design.
    \item \textbf{Wider CNN Inefficiency:} The Wider CNN, despite requiring the longest training time, yielded the lowest accuracy among custom variants, indicating that simply increasing network width was not an efficient strategy here.
    \item \textbf{Regularization Impact:} The Regularized CNN underperformed relative to the Base CNN, implying that the applied regularization might have been overly restrictive, potentially hindering the model's learning capacity.
\end{itemize}

\subsubsection{Architectural Insights}
This comparative analysis yielded several valuable insights regarding architectural choices:
\begin{itemize}
    \item \textbf{Depth vs. Width:} Increasing network depth proved more beneficial (Deeper: 86.67\%) than increasing width (Wider: 80.67\%), highlighting the importance of hierarchical feature extraction enabled by depth.
    \item \textbf{Regularization Balance:} The results underscore the need for a delicate balance in regularization; the heavily regularized model underperformed, emphasizing the importance of matching regularization strength to model capacity and task complexity.
    \item \textbf{Precision-Recall Tradeoff:} The Deeper CNN exhibited the most favorable balance between precision and recall among custom variants, suggesting a more robust learning process.
    \item \textbf{Complexity vs. Performance:} Although the Deeper CNN required slightly longer training, its superior accuracy justified the additional complexity, indicating effective utilization of architectural depth.
\end{itemize}
Based on these comprehensive findings, the Deeper CNN architecture was selected as the candidate for targeted optimization in the final phase.

\subsection{Phase 3 Results: Deeper CNN Optimization}
Finally, Phase 3 focused on enhancing the performance of the selected Deeper CNN architecture through a systematic, iterative optimization approach conducted in two distinct subphases.

\subsubsection{Phase 3.1: Initial Optimization Results and Analysis}
Contrary to expectations, our first optimization attempt actually resulted in a performance regression. Despite implementing several advanced techniques (residual connections, AdamW optimizer, OneCycleLR scheduling), the model achieved only 81\% test accuracy—a 5.67\% decrease from the original Deeper CNN's 86.67\%.

The learning curves revealed several insightful patterns:
\begin{itemize}
    \item \textbf{Continuous Improvement}: Both training and validation losses decreased consistently throughout all 75 epochs, with no signs of overfitting.
    \item \textbf{Training-Validation Gap}: A substantial disparity emerged between training accuracy (approximately 61\%) and validation accuracy (approximately 87\%), suggesting potential underfitting rather than overfitting.
    \item \textbf{Class-Specific Challenges}: Notably poor performance on wind instruments, with Didgeridoo (33\% F1-score), Flute (55\% F1-score), and Trombone (60\% F1-score) showing particularly weak recognition.
\end{itemize}

Detailed analysis uncovered several probable causes for this unexpected performance decline:
\begin{itemize}
    \item \textbf{Over-regularization}: The combined effect of residual connections, progressive dropout (0.1–0.5), and relatively high weight decay (0.001) likely constrained the model's learning capacity excessively.
    \item \textbf{Suboptimal Augmentation Strategy}: The decision to reduce augmentation strength from 'optimized' to 'medium' may have limited the model's exposure to useful variations.
    \item \textbf{Negative Parameter Interactions}: The specific combination of learning rate (max\_lr=0.003), architecture modifications, and regularization techniques created unfavorable dynamics that hindered effective training.
    \item \textbf{Training-Validation Gap}: A substantial disparity between training accuracy (approximately 61\%) and validation accuracy (87.33\%) suggested significant underfitting rather than overfitting.
    \item \textbf{Class-Specific Challenges}: Particularly poor performance on specific instruments—Didgeridoo (33\% F1-score), Flute (55\% F1-score), and Trombone (60\% F1-score)—indicated potential structural issues in the model's ability to capture certain visual features.
\end{itemize}

This unexpected result highlighted the complex, often unpredictable interactions between deep learning optimization techniques and underscored the necessity of empirical validation rather than sole reliance on theoretical best practices.

\subsubsection{Phase 3.2: Refined Optimization Results}
Learning from the Phase 3.1 setback, we implemented a substantially refined optimization strategy that yielded dramatically improved results. The optimized Deeper CNN achieved a final test accuracy of 93.33\%. This represents a substantial improvement of 6.66\% compared to its original Phase 2 performance (86.67\%) and a remarkable 12.33\% improvement over the first optimization attempt (81\%).

Table II summarizes the key performance metrics for the optimized models across both optimization subphases.

\begin{table}[ht]
\caption{Performance Comparison of Deeper CNN Optimization Attempts}
\centering
\begin{tabular}{lccccc}
\toprule
\textbf{Model} & \textbf{Test Acc.} & \textbf{Val Acc.} & \textbf{Training Time} & \textbf{Parameters} \\
\midrule
ResNet-18 (Baseline) & 99.33\% & 100.00\% & 32.48 min & 11.7M \\
Original Deeper CNN & 86.67\% & 90.67\% & 36.06 min & 9.2M \\
Phase 3.1 Optimization & 81.00\% & 87.33\% & 42.27 min & 13.6M \\
\textbf{Phase 3.2 Optimization} & \textbf{93.33\%} & \textbf{96.67\%} & 42.77 min & 15.0M \\
\bottomrule
\end{tabular}
\end{table}

\subsubsection{Impact of Optimization Techniques and Iterative Approach}
The dramatic performance improvement in Phase 3.2 resulted from applying key lessons learned from the Phase 3.1 setback. The successful optimizations were built on failure analysis and strategic adjustments:

\begin{itemize}
    \item \textbf{Balanced Regularization:} Phase 3.1's over-regularization was addressed through a gentler graduated dropout strategy ([0.05, 0.1, 0.15, 0.2, 0.25, 0.3] versus [0.1, 0.2, 0.3, 0.4, 0.4, 0.5]) and reduced weight decay (0.0005 versus 0.001), achieving better balance between regularization and learning capacity.
    
    \item \textbf{Selective Feature Enhancement:} Rather than applying attention mechanisms universally, Phase 3.2 selectively implemented them only in deeper layers (layers 4-6), allowing early layers to focus on general feature extraction without interference.
    
    \item \textbf{Class-Specific Augmentation:} Analysis of Phase 3.1's poor performance on specific instruments (Didgeridoo, Flute, etc.) led to targeted augmentation strategies for challenging classes, substantially improving their recognition. This approach proved particularly effective for wind instruments with distinctive shapes (Didgeridoo F1-score: 33% → 75%, Flute: 55% → 60%).
    
    \item \textbf{Refined Training Protocol:} The training regimen was calibrated with a more appropriate OneCycleLR learning rate schedule (max\_lr=0.001, pct\_start=0.25), increased gradient clipping threshold (2.0), and reduced label smoothing (0.05), creating more stable optimization dynamics.
    
    \item \textbf{Mixed Precision Training:} This technique accelerated the experimental cycle, enabling more thorough exploration and tuning of hyperparameters within comparable training time (42.77 min versus 42.27 min for Phase 3.1).
\end{itemize}

This iterative approach highlights a fundamental aspect of deep learning research: systematic analysis of failures often provides more valuable insights than immediate successes. The Phase 3.1 setback, rather than representing a dead end, served as a crucial stepping stone that informed the highly successful Phase 3.2 optimizations.

% TODO: Insert Figure X: Comparison of per-class F1 scores before and after Phase 3 optimization.

\subsubsection{Analysis of Challenging Classes}
Despite the significant overall improvement, certain instrument classes remained relatively challenging. Analysis of the confusion matrix (Figure Y) reveals some lingering confusion between visually similar instruments (e.g., certain guitars, woodwinds), although the severity was reduced compared to Phase 2.

% TODO: Insert Figure Y: Confusion matrix for the optimized Deeper CNN.

Notably, the F1 scores for previously identified challenging classes showed marked improvements following the targeted optimization phase, validating the effectiveness of the class-specific augmentation strategy. The performance analysis revealed:

\begin{itemize}
    \item \textbf{Perfect Classification (100\% F1-score):} Twenty instruments achieved perfect recognition, including Tambourine, Xylophone, Accordion, Bagpipes, Banjo, Bongo drum, Piano, Saxophone, Trombone, Tuba, and Violin, demonstrating the model's enhanced capability to identify diverse instrument types.
    
    \item \textbf{Improved Recognition:} Previously challenging instruments showed substantial improvements, such as Didgeridoo (33\% → 75\% F1-score), Trombone (60\% → 100\%), and Sitar (62\% → 89\%).
    
    \item \textbf{Persistent Challenges:} Two instruments remained particularly difficult: Alphorn (57\% F1-score, showing 100\% precision but only 40\% recall) and Flute (60\% F1-score), both sharing characteristics of being long, thin wind instruments with similar visual profiles.
\end{itemize}

The pattern of remaining misclassifications reveals an interesting insight: instruments with long, cylindrical shapes and limited distinctive visual features pose the greatest challenge even for an optimized model. This suggests that for these specific categories, additional techniques such as multi-view classification or specialized feature extraction might be necessary for further improvements.

\subsubsection{Discussion}
The results from Phase 3 demonstrate that systematic and targeted optimization can significantly elevate the performance of custom CNN architectures. The integration of techniques including selective attention, residual connections, advanced regularization, and class-specific augmentation successfully narrowed the performance discrepancy compared to the strong transfer learning baseline. This highlights the potential for custom-designed models to achieve highly competitive results in specialized domains, provided that dedicated refinement and optimization efforts are undertaken.
