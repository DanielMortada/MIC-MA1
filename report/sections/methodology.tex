% =====================================
% Methodology Section
% File: sections/methodology.tex
% =====================================

\section{Methodology}
This section delineates the three-phase methodology employed for the task of musical instrument classification, encompassing transfer learning, custom CNN architecture development, and subsequent systematic optimization.

\subsection{Phase 1: Comparative Analysis of Transfer Learning and Custom CNN}
Our initial phase focused on establishing a robust performance baseline via transfer learning, while concurrently developing a bespoke custom CNN architecture.

\subsubsection{Transfer Learning using ResNet-18}
We selected the well-established ResNet-18 architecture \cite{he2016deep} as our baseline model, primarily due to its proven efficacy across diverse computer vision tasks and its inherent architectural efficiency. Notably, its incorporation of residual connections facilitates gradient flow, rendering it particularly suitable for transfer learning applications.

Our transfer learning procedure involved the following steps:
\begin{itemize}
    \item Initialization of the network with weights pre-trained on the ImageNet dataset.
    \item Replacement of the final fully-connected layer with a new layer comprising 30 output units, corresponding to the instrument classes.
    \item Fine-tuning of the entire network using a reduced learning rate to adapt the pre-learned features to the specific nuances of the target task.
\end{itemize}
ResNet-18's efficiency and residual connections effectively mitigate the vanishing gradient problem, thereby enabling successful fine-tuning across the network depth.

% TODO: Insert diagram showing ResNet-18 architecture

\subsubsection{Custom CNN Architecture Development}
As an alternative to leveraging pre-trained models, we developed a custom CNN architecture specifically tailored for musical instrument classification. This architecture adopts a pyramidal structure, characterized by progressively increasing channel depth and decreasing spatial dimensions—a common design pattern in effective CNNs.

The custom CNN comprises five sequential convolutional blocks:
\begin{itemize}
    \item \textbf{Block 1:} 3 → 32 → 32 channels (designed for basic feature detection, e.g., edges, textures)
    \item \textbf{Block 2:} 32 → 64 → 64 channels
    \item \textbf{Block 3:} 64 → 128 → 128 channels
    \item \textbf{Block 4:} 128 → 256 → 256 channels
    \item \textbf{Block 5:} 256 → 512 → 512 channels (designed for complex, high-level features)
\end{itemize}
Each block consists of two convolutional layers, incorporating batch normalization and ReLU activation functions, followed by max pooling for spatial downsampling and dropout for regularization. This hierarchical structure allows the network to construct increasingly complex feature representations.

Subsequent to the convolutional blocks, global average pooling reduces the feature maps into a 512-dimensional vector. This vector is then processed by a classifier head, which includes a hidden layer, dropout, and a final output layer with 30 units.

Key design principles underpinning this custom architecture include:
\begin{itemize}
    \item \textbf{Progressive Feature Extraction:} Increasing channel depth facilitates the learning of a feature hierarchy.
    \item \textbf{Balanced Network Depth:} The architecture aims for sufficient depth to capture complexity without inducing significant optimization challenges.
    \item \textbf{Regularization:} Batch normalization and dropout are employed to mitigate overfitting.
    \item \textbf{Parameter Efficiency:} Utilization of small 3×3 kernels and controlled channel progression promotes computational efficiency.
    \item \textbf{Global Average Pooling:} This layer reduces the number of parameters compared to traditional fully-connected layers and enhances translation invariance.
\end{itemize}

% TODO: Insert diagram illustrating the custom CNN architecture

\subsection{Phase 2: Flexible Experimental Framework and Architectural Comparison}
Building upon the initial findings, Phase 2 broadened the investigation to systematically explore multiple model architectures. To facilitate this, we developed a flexible experimental framework designed for rigorous, controlled comparisons among different CNN variants.

\subsubsection{Flexible Experimental Framework Design}
This framework was engineered to ensure that performance differences could be primarily attributed to architectural modifications rather than confounding variables. Key features include:
\begin{itemize}
    \item Consistent data loading and preprocessing pipelines across all experiments.
    \item Standardized training procedures with easily configurable hyperparameters.
    \item Uniform evaluation metrics and integrated visualization tools.
    \item Automated logging of experimental results for streamlined comparative analysis.
    \item Modular architecture definition managed through configuration files.
\end{itemize}
This systematic methodology allows for reliable attribution of performance variations to specific design choices, drawing inspiration from established architecture search practices \cite{zoph2018learning, tan2019efficientnet}.

\subsubsection{Investigated Architecture Variants}
Within Phase 2, we evaluated several architectural variations derived from our base custom CNN:
\begin{itemize}
    \item \textbf{Base Custom CNN:} The original architecture developed in Phase 1.
    \item \textbf{Deeper CNN:} An extension of the base model with additional convolutional blocks to increase network depth.
    \item \textbf{Wider CNN:} A variant with an increased number of filters (channels) in each convolutional layer.
    \item \textbf{Regularized CNN:} The base model incorporating supplementary regularization techniques.
    \item \textbf{ResNet-18:} Maintained throughout as a consistent performance baseline.
\end{itemize}
For each variant, we meticulously analyzed training dynamics, convergence characteristics, final accuracy, and computational overhead to identify the most promising architectural motifs for this specific classification task.

% TODO: Insert table or diagram comparing architecture variants

\subsection{Phase 3: Optimization of the Deeper CNN}
The culmination of our project, Phase 3, centered upon the targeted optimization of the Deeper CNN architecture, identified as the most effective custom model in Phase 2. Based on the preceding analysis, we devised a systematic optimization strategy addressing four key areas:
\begin{itemize}
    \item \textbf{Enhanced Data Augmentation:} Implementation of class-specific augmentation strategies, particularly focusing on instrument classes identified as challenging, utilizing balanced transformation techniques.
    \item \textbf{Architecture Refinements:} Integration of selective attention mechanisms (primarily in deeper layers), refinement of the graduated dropout strategy, and incorporation of residual connections to improve gradient flow.
    \item \textbf{Training Strategy Optimization:} Adoption of the AdamW optimizer, utilization of the OneCycleLR learning rate scheduler, application of gradient clipping, and leveraging mixed precision training.
    \item \textbf{Advanced Experimental Techniques:} Exploration of label smoothing, extended training durations coupled with robust early stopping criteria, and fine-grained hyperparameter optimization.
\end{itemize}
This multi-faceted optimization strategy was applied incrementally, allowing for careful monitoring of validation performance and informed adjustments at each stage, with the overarching goal of maximizing the Deeper CNN's performance.

\subsubsection{Enhanced Architecture Components}
Specific architectural enhancements incorporated during this phase included:
\begin{itemize}
    \item \textbf{Residual Connections:} Skip connections were integrated between convolutional blocks to enhance gradient propagation throughout the deep network, adapting principles from ResNet architectures.
    \item \textbf{Graduated Dropout Strategy:} A progressive dropout schedule (e.g., [0.1, 0.2, ..., 0.5]) was implemented, increasing the dropout rate in deeper layers to provide intensified regularization where potentially most needed.
    \item \textbf{Selective Attention Mechanism:} Attention mechanisms were strategically applied (enabled via the `enhanced_flexible_cnn` configuration) to potentially allow the network to focus computational resources on the most informative high-level features.
\end{itemize}

\subsubsection{Advanced Training Configuration Details}
Our optimized training configuration incorporated several advanced techniques:
\begin{itemize}
    \item \textbf{AdamW Optimizer:} Selected for its improved handling of weight decay compared to standard Adam, utilized with a decay value of 0.001.
    \item \textbf{OneCycleLR Scheduler:} Implemented to potentially achieve faster yet stable convergence, configured with a maximum learning rate of 0.01 and dedicating 30\% of iterations to an initial warmup phase (`pct_start=0.3`).
    \item \textbf{Gradient Clipping:} Applied with a maximum norm threshold of 1.0 to mitigate the risk of gradient explosion and further stabilize the training dynamics.
    \item \textbf{Mixed Precision Training:} Leveraged to accelerate computations by performing suitable operations in half-precision (FP16), thereby reducing memory footprint and potentially speeding up training.
    \item \textbf{Training Duration and Early Stopping:} Configured for a maximum of 75 epochs, but regulated by an early stopping mechanism (patience=15) based on validation performance to prevent overfitting.
    \item \textbf{Label Smoothing:} Applied with a smoothing factor of 0.1 to regularize the model by discouraging overconfident predictions.
\end{itemize}

\subsubsection{Class-Specific Augmentation Strategy}
Recognizing from prior analyses that certain classes (e.g., Alphorn, Flute, Clarinet, Didgeridoo) posed persistent classification challenges, a specialized augmentation strategy (`augmentation_strength: optimized`) was devised:
\begin{itemize}
    \item \textbf{Targeted Transformations:} Stronger, yet carefully controlled, augmentation transformations were applied specifically to images belonging to these identified challenging classes, while a more moderate augmentation regime was maintained for classes already classified with high accuracy. (Specific parameters, such as crop scale 0.8-1.0 and rotation ±15°, are implementation details within this strategy).
\end{itemize}
