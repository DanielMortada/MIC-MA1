% =====================================
% Methodology Section
% File: sections/methodology.tex
% =====================================

\section{Methodology}
\noindent
Our three‑phase pipeline moves from a transfer‑learning baseline to bespoke architectures and targeted optimisation.

\subsection{Phase~1 – Transfer Learning vs. Custom CNN}
\noindent
\textbf{Baseline.} A ResNet‑18 pre‑trained on ImageNet is fine‑tuned by replacing the final layer with 30 outputs and training the whole network at a reduced learning rate (\autoref{fig:resnet18_arch}).
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\columnwidth, height = 3cm]{figures/resnet18_architecture.png}
    \caption{ResNet-18 architecture with residual connections. The diagram illustrates the network's 18-layer structure with identity shortcut connections that facilitate gradient flow during training. The final fully-connected layer is replaced with a 30-output layer corresponding to the instrument classes for fine-tuning in our task \cite{resnet18_fig}.}
    \label{fig:resnet18_arch}
\end{figure}\\  
\textbf{Custom.} A five‑block CNN (channels: 3‑32‑64‑128‑256‑512) with $3 \times 3$ kernels, batch normalisation, ReLU, max‑pool, dropout, global‑average pooling, and a 30‑way classifier is trained from scratch (\autoref{fig:pyramid_cnn}).

% Place this in sections/figures.tex or directly where you need the figure
% Requires \usepackage{tikz}

% Add this to your preamble

% -------------------------------------
% Vertical pyramidal CNN figure
% -------------------------------------

\begin{figure}[h!]
    \centering
    \rotatebox{90}{ % rotate 90 degrees
        \includegraphics[width=0.35\linewidth]{figures/pyramid_cnn.png}
    }
    \caption{Pyramidal custom CNN architecture. Five convolutional blocks progressively increase channels, followed by global average pooling and a 30-class classifier. Colors indicate block types: input (blue), convolution (orange), pooling (green), dense (purple).}
    \label{fig:pyramid_cnn}
\end{figure}


\subsection{Phase~2 – Framework and Architecture Survey}
\noindent
A common PyTorch framework fixes data loading, augmentation, optimiser, and metrics, ensuring that accuracy differences arise only from architecture. Variants examined (as illustrated in Table \ref{tab:cnn_variants}) are:  
(i) Base CNN, (ii) Deeper (+ extra blocks), (iii) Wider (more filters), (iv) Regularised (extra dropout/\!weight decay), and (v) ResNet‑18 as reference. Training dynamics, accuracy, and compute cost guide selection.

\begin{table}[htbp]
\caption{Architectural Comparison of CNN Variants}
\label{tab:cnn_variants}
\centering
\begin{tabular}{lcccc}
\toprule
\textbf{Feature} & \textbf{Base CNN} & \textbf{Deeper CNN} & \textbf{Wider CNN} & \textbf{Regularised CNN} \\
\midrule
Conv. Blocks & 5 & 7 & 5 & 5 \\
Final Channels & 512 & 512 & 1024 & 512 \\
Layers & 10 & 14 & 10 & 10 \\
Dropout Rate & 0.25 & 0.25 & 0.25 & 0.4 \\
Weight Decay & 1e-4 & 1e-4 & 1e-4 & 1e-3 \\
Parameters (M) & 8.6 & 9.2 & 22.1 & 8.6 \\
\bottomrule
\end{tabular}
\end{table}

\subsection{Phase~3 – Deeper CNN Optimisation}
\noindent
The best custom model (Deeper CNN) is refined systematically through an iterative optimization approach.

\subsubsection{Phase~3.1 – Initial Optimization Approach}
\noindent
Building on architectural insights from ResNet \cite{he2016deep}, we implemented several advanced features:

\begin{itemize}
    \item \textbf{Architecture.} Residual connections to improve gradient flow through the network, enhancing training of deeper layers.
    \item \textbf{Regularization.} Progressive dropout schedule (0.1--0.5) that increases with network depth, providing stronger regularization in later, more specialized layers.
    \item \textbf{Optimization.} AdamW optimizer with weight decay 0.001 (decoupled from gradient updates \cite{loshchilov2017decoupled}), OneCycleLR scheduling with maximum learning rate 0.003, and gradient clipping at norm 1.0 to stabilize training.
    \item \textbf{Training.} Reduced augmentation strength to preserve key structural features of instruments, extended training to 75 epochs with increased early stopping patience (25 epochs).
\end{itemize}

\noindent
This approach aimed to balance improved gradient flow through the network with appropriate regularization to prevent overfitting.

\subsubsection{Phase~3.2 – Failure-Driven Refinement}
\noindent
Systematic analysis of Phase~3.1 failures revealed an over-regularization problem, leading to carefully calibrated adjustments:

\begin{itemize}
    \item \textbf{Augmentation.} Class-specific transforms for challenging instruments (wind instruments and those with complex shapes), including targeted random cropping with scale factors 0.8--1.0, more conservative rotation ($\pm 15^\circ$ instead of $\pm 30^\circ$), and balanced color jittering to preserve instrument details.
    
    \item \textbf{Architecture.} Selective attention mechanisms implemented only in deeper layers (4--6) to focus on semantically meaningful features while maintaining efficient computation. Gentler dropout progression (0.05--0.3) to reduce regularization pressure, with increased final convolutional width (640 channels) for richer feature representation.
    
    \item \textbf{Training.} Fine-tuned optimization with AdamW (weight decay reduced to 0.0005), OneCycleLR with modified parameters (max\_lr 0.001, pct\_start 0.25), increased gradient clipping threshold (2.0), mixed precision training for computational efficiency, extended training to 100 epochs with early stopping patience of 25 epochs, mild label smoothing (0.05) to improve generalization, and increased batch size to 64.
\end{itemize}

\noindent
Figure~\ref{fig:deeper_cnn_arch} illustrates the architectural evolution from Phase~3.1 to Phase~3.2.

\begin{figure}[htbp]
    \centering
    % This is a placeholder for the architectural comparison figure
    % Replace with actual figure when available
    \caption{Architectural evolution of the Deeper CNN model. Phase~3.1 implemented uniform residual connections throughout the network, while Phase~3.2 introduced selective attention mechanisms in deeper layers (4--6) and employed a more graduated dropout pattern. Colors represent different components: convolutional layers (orange), residual connections (blue), attention mechanisms (green), and dropout (red, with intensity representing dropout rate).}
    \label{fig:deeper_cnn_arch}
\end{figure}

\subsection{Implementation Details}
\noindent
All experiments use cross‑entropy loss, batch size 32 (64 in Phase~3.2), ImageNet normalisation, and early stopping (patience 10 unless stated). Custom models start from He initialisation \cite{he2015delving}; all code runs on a single GPU (Google Collab's T4).
